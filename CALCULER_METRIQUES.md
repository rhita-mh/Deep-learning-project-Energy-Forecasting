# ğŸ“‹ Guide Ã‰tape par Ã‰tape - Calculer les MÃ©triques RÃ©elles

## ğŸ¯ Objectif
Calculer les vraies mÃ©triques (RMSE, MAE, RÂ²) de tous les modÃ¨les sur le test set pour remplacer les estimations.

---

## âœ… Ã‰TAPE 1 : Ouvrir Anaconda Prompt

1. Appuyez sur la touche **Windows**
2. Tapez **"Anaconda Prompt"**
3. Cliquez sur **"Anaconda Prompt"**

---

## âœ… Ã‰TAPE 2 : Activer l'environnement tf_clean

Dans Anaconda Prompt, tapez :

```bash
conda activate tf_clean
```

**RÃ©sultat attendu** : Vous devriez voir `(tf_clean)` au dÃ©but de la ligne.

---

## âœ… Ã‰TAPE 3 : Aller dans le dossier du projet

Tapez :

```bash
cd C:\Users\asus\Desktop\cur
```

---

## âœ… Ã‰TAPE 4 : VÃ©rifier que les modÃ¨les existent

Tapez :

```bash
dir models
```

**Fichiers attendus** (7 fichiers) :
- âœ… `scaler.pkl`
- âœ… `params.pkl`
- âœ… `decision_tree.pkl`
- âœ… `mlp_model.h5`
- âœ… `cnn_model.h5`
- âœ… `lstm_uni_model.h5`
- âœ… `lstm_multi_model.h5`

**Si tous ces fichiers sont prÃ©sents** â†’ Passez Ã  l'Ã©tape 5
**Si des fichiers manquent** â†’ Vous devrez rÃ©-entraÃ®ner (Ã©tape 5 prendra plus de temps)

---

## âœ… Ã‰TAPE 5 : ExÃ©cuter le script d'entraÃ®nement

Tapez :

```bash
python train_models.py
```

**Ce qui va se passer** :
1. âœ… Chargement des donnÃ©es
2. âœ… PrÃ©paration des donnÃ©es
3. âœ… EntraÃ®nement des modÃ¨les (si nÃ©cessaire)
4. âœ… **NOUVEAU** : Calcul des mÃ©triques rÃ©elles sur le test set
5. âœ… Sauvegarde des mÃ©triques dans `models/model_metrics.pkl`

â±ï¸ **Temps estimÃ©** :
- Si les modÃ¨les existent dÃ©jÃ  : **5-10 minutes** (juste le calcul des mÃ©triques)
- Si les modÃ¨les n'existent pas : **15-30 minutes** (entraÃ®nement complet)

**âœ… VÃ©rification** : Ã€ la fin, vous devriez voir :
```
MÃ©triques calculÃ©es sur le test set:
============================================================
Decision Tree:
  RMSE: XXX.XX
  MAE:  XXX.XX
  RÂ²:   X.XXXX
...
============================================================
âœ“ TOUS LES MODÃˆLES ONT Ã‰TÃ‰ ENTRÃ‚INÃ‰S ET SAUVEGARDÃ‰S
âœ“ MÃ‰TRIQUES CALCULÃ‰ES ET SAUVEGARDÃ‰ES
```

---

## âœ… Ã‰TAPE 6 : VÃ©rifier que les mÃ©triques sont sauvegardÃ©es

Tapez :

```bash
dir models
```

**Nouveau fichier attendu** :
- âœ… `model_metrics.pkl` (nouveau fichier)

Si ce fichier est prÃ©sent, les mÃ©triques rÃ©elles ont Ã©tÃ© calculÃ©es !

---

## âœ… Ã‰TAPE 7 : RedÃ©marrer l'application Streamlit

**Option A : Si l'application est dÃ©jÃ  ouverte**
1. Dans la fenÃªtre oÃ¹ Streamlit tourne, appuyez sur **Ctrl+C** pour l'arrÃªter
2. Puis tapez : `streamlit run app.py`

**Option B : Si l'application n'est pas ouverte**
Tapez simplement :

```bash
streamlit run app.py
```

---

## âœ… Ã‰TAPE 8 : VÃ©rifier dans l'application

1. **Ouvrez la page "ğŸ¯ Performances des ModÃ¨les"**
2. **SÃ©lectionnez "LSTM (Multivariate)"** dans Deep Learning Models
3. **VÃ©rifiez les mÃ©triques** : Elles devraient maintenant Ãªtre les vraies valeurs calculÃ©es

4. **Allez dans "âš–ï¸ Comparaison des ModÃ¨les"**
5. **Lisez la note explicative** qui explique pourquoi une prÃ©diction unique peut diffÃ©rer du RMSE global

---

## ğŸ¯ RÃ©sultat Attendu

### Avant (Estimations) :
- LSTM (Multivariate): RMSE: 155.0 (estimation)

### AprÃ¨s (MÃ©triques RÃ©elles) :
- LSTM (Multivariate): RMSE: [valeur rÃ©elle calculÃ©e sur le test set]

Les mÃ©triques seront maintenant **cohÃ©rentes** et reflÃ©teront la vraie performance de chaque modÃ¨le.

---

## âš ï¸ Notes Importantes

1. **Une seule prÃ©diction â‰  Performance globale**
   - Le RMSE est calculÃ© sur 10,000+ prÃ©dictions
   - Une date spÃ©cifique peut avoir une erreur diffÃ©rente
   - C'est normal qu'un modÃ¨le excellent en moyenne ait parfois de mauvaises prÃ©dictions

2. **Les modÃ¨les LSTM sont gÃ©nÃ©ralement meilleurs**
   - Ils ont le meilleur RMSE global
   - Mais peuvent avoir des erreurs sur certaines dates spÃ©cifiques
   - C'est pourquoi vous voyez parfois une mauvaise prÃ©diction sur une date

3. **Pour voir la vraie performance** :
   - Consultez la page "Performances des ModÃ¨les" (mÃ©triques globales)
   - La page "Comparaison avec Valeurs RÃ©elles" montre seulement une prÃ©diction unique

---

## ğŸ“ Commandes RÃ©sumÃ©es

```bash
# 1. Activer l'environnement
conda activate tf_clean

# 2. Aller dans le dossier
cd C:\Users\asus\Desktop\cur

# 3. Calculer les mÃ©triques (et entraÃ®ner si nÃ©cessaire)
python train_models.py

# 4. Lancer l'application
streamlit run app.py
```

---

## â“ ProblÃ¨mes Possibles

### âŒ "FileNotFoundError: models/scaler.pkl"
**Solution** : Les modÃ¨les n'existent pas. L'Ã©tape 5 va les crÃ©er (cela prendra 15-30 minutes).

### âŒ "Module not found"
**Solution** : VÃ©rifiez que vous Ãªtes dans l'environnement tf_clean :
```bash
conda activate tf_clean
pip install -r requirements.txt
```

### âŒ L'application ne charge pas les nouvelles mÃ©triques
**Solution** : 
1. ArrÃªtez l'application (Ctrl+C)
2. Supprimez le cache : `rm -rf .streamlit/cache` (ou supprimez le dossier manuellement)
3. Relancez : `streamlit run app.py`

---

**C'est tout ! Suivez ces Ã©tapes dans l'ordre et vous aurez les mÃ©triques rÃ©elles ! ğŸš€**

