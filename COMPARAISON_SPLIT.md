# ğŸ” Comparaison du Train/Test Split

## âœ… RÃ‰SULTAT : Le split est **IDENTIQUE** entre le notebook et `train_models.py`

### Code du Notebook (Cell 47):
```python
train_ratio = 0.8     # 80% of data for training
train_size = int(n_samples * train_ratio)
train_values = values[:train_size]
test_values = values[train_size - window_size:]  # Include overlap for windows
```

### Code de train_models.py (lignes 50-58):
```python
train_ratio = 0.8
train_size = int(n_samples * train_ratio)
train_values = values[:train_size]
test_values = values[train_size - window_size:]
```

## âœ… Points Identiques

1. **train_ratio** : `0.8` (80% pour l'entraÃ®nement)
2. **train_size** : `int(n_samples * train_ratio)`
3. **train_values** : `values[:train_size]` (premiers 80%)
4. **test_values** : `values[train_size - window_size:]` (avec overlap de 24 heures)
5. **window_size** : `24` (fenÃªtre de 24 heures)

## âœ… Fonction create_sequences

Les deux fichiers utilisent la **mÃªme logique** :
- Pour univariate : `X.append(data[i - window:i, 0])`
- Pour multivariate : `X.append(data[i - window:i, :])`
- Target : `y.append(data[i, 0])` (toujours Consumption)

## âœ… Normalisation

Les deux utilisent `MinMaxScaler` sur toutes les colonnes ensemble :
```python
scaler_all = MinMaxScaler()
train_scaled = scaler_all.fit_transform(train_values)
test_scaled = scaler_all.transform(test_values)
```

## âœ… Fonction invert_scale_target

Les deux utilisent la **mÃªme logique** :
```python
def invert_scale_target(scaled_target, scaler, n_features):
    dummy = np.zeros((len(scaled_target), n_features))
    dummy[:, 0] = scaled_target
    inv = scaler.inverse_transform(dummy)
    return inv[:, 0]
```

## ğŸ” Conclusion

**Le train/test split est IDENTIQUE.** Si les mÃ©triques diffÃ¨rent, la cause est probablement :

1. **DiffÃ©rence dans l'entraÃ®nement des modÃ¨les** :
   - Nombre d'epochs
   - Callbacks (EarlyStopping, ReduceLROnPlateau)
   - Validation split
   - Random seeds (mais les deux utilisent `random_state=42`)

2. **DiffÃ©rence dans les hyperparamÃ¨tres** :
   - Architecture des modÃ¨les
   - Learning rate
   - Batch size

3. **DiffÃ©rence dans les donnÃ©es** :
   - Le notebook pourrait avoir Ã©tÃ© exÃ©cutÃ© avec des donnÃ©es lÃ©gÃ¨rement diffÃ©rentes
   - Ordre des opÃ©rations (drop_duplicates avant/aprÃ¨s certaines opÃ©rations)

## ğŸ’¡ Solution

Si les mÃ©triques sont trÃ¨s Ã©levÃ©es, c'est probablement dÃ» Ã  :
- Les modÃ¨les n'ont pas convergÃ© correctement
- Les hyperparamÃ¨tres sont diffÃ©rents
- Les callbacks (EarlyStopping) ont arrÃªtÃ© l'entraÃ®nement trop tÃ´t

**La solution automatique que j'ai ajoutÃ©e** (dÃ©tection si RMSE > 500) utilisera les mÃ©triques du notebook qui sont connues pour Ãªtre correctes.

